{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T05:39:44.404781Z",
     "start_time": "2020-10-04T05:39:44.237318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T00:30:28.948974Z",
     "start_time": "2020-10-05T00:30:28.046878Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T00:38:47.690733Z",
     "start_time": "2020-10-05T00:38:47.687879Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.precision\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T04:04:38.768850Z",
     "start_time": "2020-10-05T04:04:14.690646Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "headers = [\n",
    "    \"Season\",\n",
    "    \"Team\",\n",
    "    \"Pos\",\n",
    "    \"G\",\n",
    "    \"GS\",\n",
    "    \"MP\",\n",
    "    \"FG\",\n",
    "    \"FGA\",\n",
    "    \"FG%\",\n",
    "    \"3P\",\n",
    "    \"3PA\",\n",
    "    \"3P%\",\n",
    "    \"2P\",\n",
    "    \"2PA\",\n",
    "    \"2P%\",\n",
    "    \"eFG%\",\n",
    "    \"FT\",\n",
    "    \"FTA\",\n",
    "    \"FT%\",\n",
    "    \"ORB\",\n",
    "    \"DRB\",\n",
    "    \"TRB\",\n",
    "    \"AST\",\n",
    "    \"STL\",\n",
    "    \"BLK\",\n",
    "    \"TOV\",\n",
    "    \"PF\",\n",
    "    \"PTS\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(columns=headers)\n",
    "advanced_df = pd.DataFrame()\n",
    "salary_df = pd.DataFrame()\n",
    "\n",
    "path = \"/usr/local/bin/chromedriver\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "url = \"https://www.basketball-reference.com/players/\"\n",
    "driver.get(url)\n",
    "\n",
    "# loop through each alphabet to click\n",
    "for page in range(1,27):\n",
    "    try:\n",
    "        # get xpath for the letters\n",
    "        letter_index = '//*[@id=\"div_alphabet\"]/ul/li[{}]/a'.format(page)\n",
    "        letter_button = driver.find_element_by_xpath(letter_index)\n",
    "\n",
    "        print(letter_button.text)\n",
    "\n",
    "        # click on the letter button\n",
    "        letter_button.click()\n",
    "        \n",
    "        # wait until the footer at the end of page is loaded\n",
    "        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID, 'footer')))\n",
    "\n",
    "        # find active players (in bold text)\n",
    "        player_list = driver.find_elements_by_xpath(\n",
    "            '//*[@id=\"players\"]/tbody/tr/th/strong'\n",
    "        )\n",
    "        print(len(player_list))\n",
    "        \n",
    "        try:\n",
    "            # loop through all of the players with specific last letter \n",
    "            for player in range(len(player_list)):\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "                # find xpath for specific player\n",
    "                player_name = driver.find_elements_by_xpath(\n",
    "                    '//*[@id=\"players\"]/tbody/tr/th/strong/a'\n",
    "                )[player]\n",
    "\n",
    "                current_player = player_name.text\n",
    "\n",
    "                # scroll down to the location of the link\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", player_name)\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "                # go to player summary info\n",
    "                player_name.click()\n",
    "                \n",
    "                # get the url for specific player\n",
    "                player_url = driver.current_url\n",
    "\n",
    "                # parse the specific page\n",
    "                soup_player = BeautifulSoup(requests.get(player_url).text, \"html.parser\")\n",
    "\n",
    "                player_stats = soup_player.find(\"tbody\")\n",
    "\n",
    "                season_played = len(soup_player.find(\"tbody\").find_all(\"tr\"))\n",
    "\n",
    "\n",
    "                if season_played > 1:\n",
    "\n",
    "                    # loop through all rows of player stats table\n",
    "                    for season in range(season_played):\n",
    "                        current_season = player_stats.find_all(\"tr\")[season].text.strip()[:7]\n",
    "                        season_stats = player_stats.find_all(\"tr\")[season].find_all(\"td\")\n",
    "                        \n",
    "                        # some player has rows with Did Not Play (played overseas)\n",
    "                        # skip those rows \n",
    "                        if season_stats[2].text.__contains__('Did Not Play'):\n",
    "                            continue\n",
    "                        \n",
    "                        season_stats = [i.text for i in (season_stats[1:2] + season_stats[3:])]\n",
    "                        season_stats = ['0.0' if i=='' else i for i in season_stats]\n",
    "                        season_stats = season_stats[:2]+[float(i) for i in season_stats[2:]]\n",
    "                        season_stats.insert(0, current_season)\n",
    "                        \n",
    "                        season_dict = dict(zip(headers, season_stats))\n",
    "                        season_dict[\"Player\"] = current_player\n",
    "\n",
    "                        # check if player was selected all stars for that season\n",
    "                        all_star = player_stats.find_all('tr')[season].find('span')\n",
    "\n",
    "                        if all_star:\n",
    "                            season_dict['All_star'] = '1'\n",
    "                        \n",
    "                        else:\n",
    "                            season_dict['All_star'] = '0'\n",
    "                        \n",
    "                        # append season statistics\n",
    "                        df = df.append(season_dict, ignore_index=True)\n",
    "                    \n",
    "                    # advanced statistics and salary data was in comments section \n",
    "                    # parse the comments section\n",
    "                    all_comments = soup_player.find_all(\n",
    "                        string=lambda text: isinstance(text, Comment)\n",
    "                    )\n",
    "\n",
    "                    for item in all_comments:\n",
    "                        # find advanced statistics\n",
    "                        if \"Advanced\" in item:\n",
    "                            adv = BeautifulSoup(item)\n",
    "\n",
    "                            playertr = adv.find(\"table\", id=\"advanced\")\n",
    "\n",
    "                            if not playertr:\n",
    "                                continue  # skip comment without table - go back to `for`\n",
    "\n",
    "                            playertr = playertr.find(\"tbody\").findAll(\"tr\")\n",
    "\n",
    "                            for row in playertr:\n",
    "                                if row:\n",
    "                                    all_td = row.find_all(\"td\")\n",
    "                                    advanced_stats = [x.text for x in all_td]\n",
    "                                    advanced_stats = ['0.0' if x=='' else x for x in advanced_stats]\n",
    "                                    curr_season = row.find_all('th')[0].text\n",
    "                                    advanced_headers = ['Season', \"Player\", \"PER\", \"TS%\", \"TRB%\", \"AST%\", \"STL%\", \"BLK%\", \"TOV%\", \"USG%\", \"OWS\", \"DWS\", \"WS/48\", \"BPM\", \"VORP\"]\n",
    "\n",
    "                                    advanced_dict = dict(\n",
    "                                        zip(\n",
    "                                            advanced_headers,\n",
    "                                            [\n",
    "                                                curr_season,\n",
    "                                                current_player,\n",
    "                                                float(advanced_stats[6]),\n",
    "                                                float(advanced_stats[7]),\n",
    "                                                float(advanced_stats[12]),\n",
    "                                                float(advanced_stats[13]),\n",
    "                                                float(advanced_stats[14]),\n",
    "                                                float(advanced_stats[15]),\n",
    "                                                float(advanced_stats[16]),\n",
    "                                                float(advanced_stats[17]),\n",
    "                                                float(advanced_stats[19]),\n",
    "                                                float(advanced_stats[20]),\n",
    "                                                float(advanced_stats[22]),\n",
    "                                                float(advanced_stats[26]),\n",
    "                                                float(advanced_stats[27])\n",
    "                                            ],\n",
    "                                        )\n",
    "                                    )\n",
    "                                    # append advanced statistics\n",
    "                                    advanced_df = advanced_df.append(\n",
    "                                        advanced_dict, ignore_index=True\n",
    "                                    )\n",
    "\n",
    "                        # get all salries data             \n",
    "                        if \"Salaries\" in item:\n",
    "                            adv = BeautifulSoup(item)\n",
    "\n",
    "                            playertr = adv.find(\"table\", id=\"all_salaries\")\n",
    "\n",
    "                            if not playertr:\n",
    "                                \n",
    "                                continue  # skip comment without table - go back to `for`\n",
    "\n",
    "                            playertr = playertr.find(\"tbody\").findAll(\"tr\")\n",
    "\n",
    "                            for row in playertr:\n",
    "                                if row:\n",
    "                                    all_td = row.find_all(\"td\")\n",
    "                                    season = row.find_all(\"th\")[0].text\n",
    "                                    salaries = all_td[2].text\n",
    "                                    salary_dict = dict(zip([\"Player\", \"Season\", \"Salary\"], [current_player, season, salaries]))\n",
    "                                    # append salary info\n",
    "                                    salary_df = salary_df.append(salary_dict, ignore_index=True)\n",
    "                     \n",
    "                        # get 2019-2020 contract\n",
    "                        if \"contract\" in item:\n",
    "                            adv = BeautifulSoup(item)\n",
    "                            try:\n",
    "                                playertr = adv.find(\"div\", id='div_contract')\n",
    "\n",
    "                                if playertr:\n",
    "                                    season_1 = playertr.find_all(\"th\")[1].text\n",
    "                                    salaries_1 = playertr.find_all('span')[1].text\n",
    "                                    contract_dict = dict(zip([\"Player\", \"Season\", \"Salary\"], [current_player, season_1, salaries_1]))\n",
    "                                    # append 2019 salary\n",
    "                                    salary_df = salary_df.append(contract_dict, ignore_index=True)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    driver.back()\n",
    "                            \n",
    "                else:\n",
    "                    driver.back()\n",
    "                        \n",
    "        except:\n",
    "            print(player, 'player not working')\n",
    "\n",
    "    except Exception:\n",
    "        print(page, 'page not working')\n",
    "\n",
    "    driver.back()\n",
    "    time.sleep(1)\n",
    "\n",
    "    final_df = pd.merge(df, salary_df, how='left')\n",
    "    final_df = final_df.merge(advanced_df, on=[\"Player\", \"Season\"]).drop_duplicates(subset=[\"Player\", \"Season\"], keep=\"first\").reset_index()\n",
    "    pickle.dump(final_df, open(\"final_{}.p\".format(page), \"wb\"))\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}